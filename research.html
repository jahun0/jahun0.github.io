<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Jahun Oh | Research</title>
  <link rel="stylesheet" href="assets/css/style.css">
</head>
<body class="research">
  <header class="site-header">
    <nav class="nav container">
      <a class="logo" href="index.html">Jahun Oh</a>
      <button class="nav-toggle" type="button" aria-expanded="false" aria-controls="nav-menu">
        <span class="nav-toggle-bar"></span>
        <span class="nav-toggle-bar"></span>
        <span class="nav-toggle-bar"></span>
        <span class="sr-only">Toggle navigation</span>
      </button>
      <div id="nav-menu" class="nav-links">
        <a href="index.html">Home</a>
        <a class="active" href="research.html">Research</a>
        <a href="publications.html">Publications</a>
        <a href="assets/cv/cv.pdf">CV</a>
      </div>
    </nav>
  </header>

  <main class="container">
    <header class="page-header">
      <h1>Research</h1>
      <p>Project-centered work focused on multi-agent decision-making, applied AI systems, and deployment-relevant evaluation.</p>
    </header>

    <section class="project-list">

      <article class="card project">
        <div class="project-header">
          <h2>Multi-Agent Reinforcement Learning for Drone Soccer</h2>
          <div class="project-links">
            <a class="pill primary" href="[ADD_PROJECT_PAGE_URL]">Project</a>
            <a class="pill" href="[ADD_CODE_URL]">Code</a>
          </div>
        </div>
        <p>
          This project explores <strong>multi-agent systems decision-making</strong> for competitive and cooperative
          robot teams in a drone-soccer environment. I study coordination under partial observability,
          non-stationarity, and resource constraints, with emphasis on scalable training and evaluation
          protocols that reflect real-world deployment challenges.
        </p>
        <h3>What I did</h3>
        <ul class="bullet-list">
          <li>Built a simulation-to-experiment workflow for multi-agent learning in drone-soccer settings</li>
          <li>Implemented baseline policies and reward designs for team coordination and role specialization</li>
          <li>Defined evaluation metrics such as team reward, stability, and adaptation under changing opponents</li>
        </ul>
        <div class="gallery-grid">
          <figure>
            <img src="assets/img/research/project-drone-1.jpg" alt="Drone soccer multi-agent simulation overview">
            <figcaption>Multi-agent drone-soccer environment.</figcaption>
          </figure>
          <figure>
            <img src="assets/img/research/project-drone-2.jpg" alt="Coordination and policy learning visualization">
            <figcaption>Coordination learning and evaluation traces.</figcaption>
          </figure>
        </div>
        <div class="project-media">
          <h3>Video</h3>
          <div class="yt" data-youtube-id="PUT_VIDEO_ID_HERE">
            <button class="yt-play" type="button">Play video</button>
          </div>
        </div>
      </article>

      <article class="card project">
        <div class="project-header">
          <h2>YouTube Sentiment Analyzer for North Korea-Related Disinformation</h2>
          <div class="project-links">
            <a class="pill primary" href="[ADD_REPORT_URL]">Report</a>
            <a class="pill" href="[ADD_CODE_URL]">Code</a>
          </div>
        </div>
        <p>
          This project develops an analyst-facing pipeline to monitor <strong>North Korea-related disinformation</strong>
          signals on YouTube by extracting video metadata and comments and performing sentiment and trend analysis.
          The goal is to support early-stage triage by surfacing shifts in narrative tone, engagement, and coordinated
          patterns across channels and time.
        </p>
        <h3>What I did</h3>
        <ul class="bullet-list">
          <li>Built a data pipeline for YouTube metadata and comment collection and normalization</li>
          <li>Implemented sentiment scoring and time-series trend analysis for narrative monitoring</li>
          <li>Designed summary dashboards to support analyst triage and reporting</li>
        </ul>
        <div class="gallery-grid">
          <figure>
            <img src="assets/img/research/project-yt-1.jpg" alt="YouTube sentiment pipeline overview">
            <figcaption>Collection → analysis → monitoring pipeline.</figcaption>
          </figure>
          <figure>
            <img src="assets/img/research/project-yt-2.jpg" alt="Sentiment and trend dashboard example">
            <figcaption>Sentiment/time-series monitoring dashboard.</figcaption>
          </figure>
        </div>
        <div class="project-media">
          <h3>Video</h3>
          <div class="yt" data-youtube-id="PUT_VIDEO_ID_HERE">
            <button class="yt-play" type="button">Play video</button>
          </div>
        </div>
      </article>

      <article class="card project">
        <div class="project-header">
          <h2>VR SBIRT Training with Conversational AI</h2>
          <div class="project-links">
            <a class="pill primary" href="https://doi.org/10.1007/978-3-031-36272-9_59">Paper</a>
            <a class="pill" href="[ADD_CODE_URL]">Code</a>
          </div>
        </div>
        <p>
          This project created a virtual reality SBIRT training environment with a conversational AI virtual patient
          for scalable clinical practice. The system integrates speech input and output with dialogue state tracking and
          policy control, enabling realistic training conversations. Results were reported at AIED 2023.
        </p>
        <h3>What I did</h3>
        <ul class="bullet-list">
          <li>Built the intent/entity pipeline and dialogue state tracking for SBIRT conversations</li>
          <li>Integrated speech I/O with Unity and a web interface for training delivery</li>
          <li>Co-authored the AIED 2023 paper and ran formative user studies</li>
        </ul>
        <div class="gallery-grid">
          <figure>
            <img src="assets/img/research/project-3-1.jpg" alt="VR SBIRT training scene">
            <figcaption>VR training environment and virtual patient.</figcaption>
          </figure>
          <figure>
            <img src="assets/img/research/project-3-2.jpg" alt="Dialogue pipeline overview">
            <figcaption>Conversational AI pipeline.</figcaption>
          </figure>
        </div>
        <div class="project-media">
          <h3>Video</h3>
          <div class="yt" data-youtube-id="PUT_VIDEO_ID_HERE">
            <button class="yt-play" type="button">Play video</button>
          </div>
        </div>
      </article>

      <article class="card project">
        <div class="project-header">
          <h2>Deep RL Offloading for Cloud Robotics Navigation</h2>
          <div class="project-links">
            <a class="pill primary" href="[ADD_THESIS_URL]">Thesis</a>
            <a class="pill" href="[ADD_CODE_URL]">Code</a>
          </div>
        </div>
        <p>
          I designed deep reinforcement learning policies for computation offloading in cloud robotics navigation.
          Using a Clearpath Jackal in ROS/Gazebo, I benchmarked DQN and Double-DQN policies against heuristic baselines
          under variable network conditions. The study quantified performance and robustness under constrained links.
        </p>
        <h3>What I did</h3>
        <ul class="bullet-list">
          <li>Implemented offloading policies and network emulation for latency and bandwidth limits</li>
          <li>Ran real-world and simulated experiments to measure navigation performance</li>
          <li>Authored the M.S. thesis on situation-aware cloud robotics navigation (2023)</li>
        </ul>
        <div class="gallery-grid">
          <figure>
            <img src="assets/img/research/project-4-1.jpg" alt="Cloud robotics navigation setup">
            <figcaption>Cloud robotics navigation testbed.</figcaption>
          </figure>
          <figure>
            <img src="assets/img/research/project-4-2.jpg" alt="Network constraint experiments">
            <figcaption>Network constraint experiments.</figcaption>
          </figure>
        </div>
        <div class="project-media">
          <h3>Video</h3>
          <div class="yt" data-youtube-id="PUT_VIDEO_ID_HERE">
            <button class="yt-play" type="button">Play video</button>
          </div>
        </div>
      </article>

    </section>
  </main>

  <footer class="site-footer">
    <div class="container">
      <p>Copyright &copy; <span id="current-year"></span> Jahun Oh. Last updated <span id="last-updated"></span>.</p>
    </div>
  </footer>

  <button id="back-to-top" type="button" aria-label="Back to top">Top</button>

  <script src="assets/js/main.js" defer></script>
</body>
</html>
